{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d4c37388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab610450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's Adventures in Wonderland\n",
      "\n",
      "                            CHAPTER I\n",
      "\n",
      "                      Down the Rabbit-Hole\n",
      "\n",
      "  Alice was beginning to get very tired of sitting by her sister\n",
      "on the bank, and o\n"
     ]
    }
   ],
   "source": [
    "corpus_path = \"data/alice.txt\"\n",
    "# corpus_path = \"data/all_en.txt\"\n",
    "# corpus_path = \"data/all_de.txt\"\n",
    "# corpus_path = \"data/all_fr.txt\"\n",
    "# corpus_path = \"data/cn/hongloumeng.txt\"\n",
    "raw_text = utils.load_text(corpus_path)\n",
    "\n",
    "# print start of the text\n",
    "print(raw_text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "85e0d2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Level=char, Punct=False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level = \"char\"  # char or word\n",
    "keep_punct = False\n",
    "order = 1\n",
    "\n",
    "# Preprocess\n",
    "reload(utils)\n",
    "print(f\"Processing: Level={level}, Punct={keep_punct}\")\n",
    "tokens = utils.clean_text(raw_text, level=level, keep_punctuation=keep_punct)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "50aafd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample counts:\n",
      "[(('a',),\n",
      "  Counter({'n': 1607,\n",
      "           't': 1167,\n",
      "           'l': 929,\n",
      "           's': 900,\n",
      "           'i': 716,\n",
      "           'r': 704,\n",
      "           ' ': 679,\n",
      "           'd': 442,\n",
      "           'y': 257,\n",
      "           'b': 214,\n",
      "           'm': 183,\n",
      "           'v': 168,\n",
      "           'g': 160,\n",
      "           'c': 157,\n",
      "           'k': 125,\n",
      "           'p': 118,\n",
      "           'u': 76,\n",
      "           'w': 75,\n",
      "           'f': 63,\n",
      "           'h': 25,\n",
      "           'j': 12,\n",
      "           'z': 5,\n",
      "           'x': 4,\n",
      "           'o': 3,\n",
      "           'a': 1})),\n",
      " (('l',),\n",
      "  Counter({'i': 855,\n",
      "           'e': 730,\n",
      "           'l': 680,\n",
      "           ' ': 670,\n",
      "           'y': 436,\n",
      "           'd': 334,\n",
      "           'o': 322,\n",
      "           'a': 308,\n",
      "           'f': 145,\n",
      "           'k': 59,\n",
      "           's': 52,\n",
      "           't': 48,\n",
      "           'w': 15,\n",
      "           'u': 14,\n",
      "           'v': 12,\n",
      "           'p': 12,\n",
      "           'm': 8,\n",
      "           'r': 4,\n",
      "           'b': 4,\n",
      "           'c': 1,\n",
      "           'g': 1}))]\n",
      "\n",
      "----- Model Statistics -----\n",
      "Order of N-gram Model (N)            : 1\n",
      "Number of Unique Contexts            : 27\n",
      "Total Observed N-grams (Transitions) : 134,074\n",
      "Unique (Context, Token) N-grams      : 456\n",
      "Conditional Vocab Size (Next Tokens) : 27\n"
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "\n",
    "# show top k items\n",
    "top_k = 2\n",
    "\n",
    "# Build Model\n",
    "counts = utils.build_ngram_counts(tokens, order)\n",
    "print(\"Sample counts:\")\n",
    "pprint(list(counts.items())[:top_k])\n",
    "\n",
    "# Print some statistics\n",
    "utils.print_model_statistics(counts, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7666d64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized to proba:\n",
      "[(('a',),\n",
      "  {' ': 0.07724687144482366,\n",
      "   'a': 0.00011376564277588168,\n",
      "   'b': 0.02434584755403868,\n",
      "   'c': 0.017861205915813423,\n",
      "   'd': 0.05028441410693971,\n",
      "   'f': 0.007167235494880546,\n",
      "   'g': 0.01820250284414107,\n",
      "   'h': 0.002844141069397042,\n",
      "   'i': 0.08145620022753129,\n",
      "   'j': 0.0013651877133105802,\n",
      "   'k': 0.01422070534698521,\n",
      "   'l': 0.10568828213879408,\n",
      "   'm': 0.020819112627986348,\n",
      "   'n': 0.18282138794084188,\n",
      "   'o': 0.00034129692832764505,\n",
      "   'p': 0.013424345847554038,\n",
      "   'r': 0.08009101251422071,\n",
      "   's': 0.10238907849829351,\n",
      "   't': 0.13276450511945392,\n",
      "   'u': 0.008646188850967008,\n",
      "   'v': 0.01911262798634812,\n",
      "   'w': 0.008532423208191127,\n",
      "   'x': 0.0004550625711035267,\n",
      "   'y': 0.029237770193401593,\n",
      "   'z': 0.0005688282138794084}),\n",
      " (('l',),\n",
      "  {' ': 0.14225053078556263,\n",
      "   'a': 0.0653927813163482,\n",
      "   'b': 0.0008492569002123143,\n",
      "   'c': 0.00021231422505307856,\n",
      "   'd': 0.07091295116772824,\n",
      "   'e': 0.15498938428874734,\n",
      "   'f': 0.03078556263269639,\n",
      "   'g': 0.00021231422505307856,\n",
      "   'i': 0.18152866242038215,\n",
      "   'k': 0.012526539278131636,\n",
      "   'l': 0.14437367303609341,\n",
      "   'm': 0.0016985138004246285,\n",
      "   'o': 0.0683651804670913,\n",
      "   'p': 0.0025477707006369425,\n",
      "   'r': 0.0008492569002123143,\n",
      "   's': 0.011040339702760084,\n",
      "   't': 0.01019108280254777,\n",
      "   'u': 0.0029723991507431,\n",
      "   'v': 0.0025477707006369425,\n",
      "   'w': 0.0031847133757961785,\n",
      "   'y': 0.09256900212314224})]\n",
      "\n",
      "Most likely token after (' ',) :\n",
      "   token     proba\n",
      "5      t  0.167740\n",
      "0      a  0.127358\n",
      "11     s  0.095878\n",
      "1      i  0.074589\n",
      "2      w  0.070574\n",
      "12     h  0.059701\n",
      "10     o  0.054171\n",
      "7      b  0.037389\n",
      "3      c  0.034207\n",
      "16     m  0.033942\n",
      "4      d  0.031404\n",
      "19     l  0.028260\n",
      "17     f  0.028070\n",
      "13     n  0.021782\n",
      "8      g  0.021782\n",
      "23     y  0.020873\n",
      "14     p  0.018714\n",
      "6      r  0.018600\n",
      "18     e  0.014281\n",
      "15     u  0.009963\n",
      "22     k  0.009925\n",
      "9      v  0.009016\n",
      "20     q  0.007046\n",
      "21     j  0.004546\n",
      "25     x  0.000114\n",
      "24     z  0.000076\n"
     ]
    }
   ],
   "source": [
    "# Normalize to Probabilities\n",
    "model = utils.normalize_to_probs(counts)\n",
    "print(\"Normalized to proba:\")\n",
    "pprint(list(model.items())[:top_k])\n",
    "\n",
    "# get context with most diverse next token options\n",
    "context = max(model.keys(), key=lambda ctx: len(model[ctx]))\n",
    "\n",
    "# show sorted next token probabilities for this context\n",
    "df = pd.DataFrame(model[context].items(), columns=[\"token\", \"proba\"]).sort_values(\n",
    "    by=\"proba\", ascending=False\n",
    ")\n",
    "print(f\"\\nMost likely token after {context} :\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "249f43aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Entropy: 3.2261 bits\n"
     ]
    }
   ],
   "source": [
    "# Calculate Entropy (Model based)\n",
    "h_model = utils.calculate_entropy_from_counts(counts)\n",
    "print(f\"Model Entropy: {h_model:.4f} bits\")\n",
    "\n",
    "# def calculate_entropy_from_counts(counts):\n",
    "#     \"\"\"\n",
    "#     H = - Sum_ctx ( P(ctx) * Sum_x ( P(x|ctx) * log2 P(x|ctx) ) )\n",
    "#     \"\"\"\n",
    "#     total_observations = sum(sum(counter.values()) for counter in counts.values())\n",
    "\n",
    "#     entropy = 0.0\n",
    "#     for context, counter in counts.items():\n",
    "#         ctx_count = sum(counter.values())\n",
    "#         p_ctx = ctx_count / total_observations\n",
    "\n",
    "#         h_cond = 0.0\n",
    "#         for token, count in counter.items():\n",
    "#             p_x_given_ctx = count / ctx_count\n",
    "#             h_cond -= p_x_given_ctx * math.log2(p_x_given_ctx)\n",
    "\n",
    "#         entropy += p_ctx * h_cond\n",
    "\n",
    "#     return entropy\n",
    "\n",
    "# The Entropy Rate (or Source Entropy) is a fundamental concept in info theory \n",
    "# that measures the avg amount of uncertainty or information per symbol (token) in a stochastic process (like text).\n",
    "# For a source that generates a sequence of symbols (e.g., characters or words)\n",
    "# the entropy rate, typically denoted $H(\\mathcal{X})$, describes how \"compressible\" the source is.\n",
    "    # Low Entropy Rate: The sequence is highly predictable (redundant). \n",
    "    # Knowing the past tokens tells you a lot about the next one.\n",
    "\n",
    "    # High Entropy Rate: The sequence is less predictable (closer to random). \n",
    "    # Knowing the past doesn't help much in predicting the next token.\n",
    "\n",
    "# -> What it is and what it is not:\n",
    "# ✅ It is H(X_t | X_{t-2},X_{t-1}) for your empirical trigram distribution (with whatever tokenization and handling of boundaries you used).\n",
    "# ❌ It is not the entropy of English in general, and it is not the unconditional entropy of the corpus.\n",
    "# ❌ It is not a guaranteed measure of how well you’ll predict new text. \n",
    "# For held-out data you’d want cross-entropy / perplexity on a test set, because the training-set conditional entropy can be optimistically low \n",
    "# (especially if there are many rare contexts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca59caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Entropy (Sequence based - NLL)\n",
    "# h_seq = utils.calculate_nll(tokens, model, order)\n",
    "# print(f\"Sequence NLL Entropy: {h_seq:.4f} bits\")\n",
    "\n",
    "# When both functions are calculated using the same sequence (i.e., the training sequence is passed as the sequence argument to calculate_nll),\n",
    "# they will indeed give you the same numerical result. \n",
    "# $$H_{\\text{model}} = L_{\\text{train}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "30929993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Text ---\n",
      "ic and was pard the came the see had of shed meaning said the were you tell added the fear about her all refree in sting tone or you know she melay want direconting here was gave of the ran tong at shaps\n"
     ]
    }
   ],
   "source": [
    "# Generate Text\n",
    "gen_len = 200\n",
    "generated_tokens = utils.generate_text(model, order, gen_len)\n",
    "\n",
    "if level == \"char\":\n",
    "    gen_text = \"\".join(generated_tokens)\n",
    "else:\n",
    "    gen_text = \" \".join(generated_tokens)\n",
    "\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a1ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chapter Analysis...\n",
      "Found 12 chapters.\n",
      "Chapter entropy saved to results/chapter_entropy.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Chapter Analysis (Extension E3) ---\n",
    "print(\"Running Chapter Analysis...\")\n",
    "chapters = utils.split_chapters(raw_text)\n",
    "print(f\"Found {len(chapters)} chapters.\")\n",
    "\n",
    "chapter_results = []\n",
    "# Analyze entropy per chapter for a fixed config (e.g., Char, Punct, k=2)\n",
    "# Ideally we train on the whole text and evaluate NLL on chapters,\n",
    "# OR train on each chapter?\n",
    "# Q = why would we want to do this ?\n",
    "\n",
    "# README says \"Evaluate non-stationarity: entropy per chapter.\"\n",
    "# Usually this means measuring the entropy OF that chapter.\n",
    "# We can measure it by:\n",
    "# 1. Training a model on that chapter and finding its entropy.\n",
    "# 2. Or using a global model and finding NLL of that chapter (surprisal).\n",
    "# \"Non-stationarity\" implies the statistics change.\n",
    "# Let's do both? Or just method 1 (simpler interpretation: \"This chapter is more complex\").\n",
    "# Method 2 (Cross-entropy) shows how much the chapter deviates from the global average.\n",
    "# Let's do Method 1 (Self-Entropy) for k=1 (simple)\n",
    "\n",
    "target_config = {\"level\": \"char\", \"keep_punct\": True, \"order\": 1}\n",
    "\n",
    "for i, chapter_text in enumerate(chapters):\n",
    "    tokens = utils.clean_text(\n",
    "        chapter_text,\n",
    "        level=target_config[\"level\"],\n",
    "        keep_punctuation=target_config[\"keep_punct\"],\n",
    "    )\n",
    "\n",
    "    if not tokens:\n",
    "        continue\n",
    "\n",
    "    k = target_config[\"order\"]\n",
    "    counts = utils.build_ngram_counts(tokens, k)\n",
    "    # model = utils.normalize_to_probs(counts) # Not needed for entropy_from_counts\n",
    "\n",
    "    h_chapter = utils.calculate_entropy_from_counts(counts)\n",
    "\n",
    "    chapter_results.append(\n",
    "        {\"chapter\": i + 1, \"entropy\": h_chapter, \"length\": len(tokens)}\n",
    "    )\n",
    "\n",
    "df_chap = pd.DataFrame(chapter_results)\n",
    "df_chap.to_csv(\"results/chapter_entropy.csv\", index=False)\n",
    "print(\"Chapter entropy saved to results/chapter_entropy.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
