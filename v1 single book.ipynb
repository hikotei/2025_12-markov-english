{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4c37388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab610450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "紅樓夢\n",
      "\n",
      "第一回　甄士隱夢幻識通靈　賈雨村風塵怀閨秀\n",
      "-----------------------------------------------------------------\n",
      "此開卷第一回也．作者自云：因曾歷過一番夢幻之后，故將真事隱去，\n",
      "而借\"通靈\"之說，撰此《石頭記》一書也．故曰\"甄士隱\"云云．但書中所記\n",
      "何事何人？自又云：“今風塵碌碌，一事無成，忽念及當日所有之女子，一\n",
      "一細考較去\n"
     ]
    }
   ],
   "source": [
    "# corpus_path = \"data/alice.txt\"\n",
    "corpus_path = \"data/cn/hongloumeng.txt\"\n",
    "# corpus_path = \"data/all_en.txt\"\n",
    "# corpus_path = \"data/all_de.txt\"\n",
    "# corpus_path = \"data/all_fr.txt\"\n",
    "raw_text = utils.load_text(corpus_path)\n",
    "\n",
    "# print start of the text\n",
    "print(raw_text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "85e0d2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Level=char, Punct=False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['d', ' ', 'd', ' ', 'z', ' ', 'b', ' ', 'o', ' ']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level = \"char\"  # char or word\n",
    "keep_punct = False\n",
    "order = 3\n",
    "\n",
    "# Preprocess\n",
    "reload(utils)\n",
    "print(f\"Processing: Level={level}, Punct={keep_punct}\")\n",
    "tokens = utils.clean_text(raw_text, level=level, keep_punctuation=keep_punct)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "50aafd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample counts:\n",
      "[(('d', ' ', 'd'), Counter({'i': 10, 'o': 7, ' ': 1, 'e': 1})),\n",
      " ((' ', 'd', ' '), Counter({'z': 1, 'f': 1, 'p': 1, 'b': 1, 'w': 1, 't': 1}))]\n",
      "\n",
      "----- Model Statistics -----\n",
      "Order of N-gram Model (N)            : 3\n",
      "Number of Unique Contexts            : 1,770\n",
      "Total Observed N-grams (Transitions) : 17,603\n",
      "Unique (Context, Token) N-grams      : 3,957\n",
      "Conditional Vocab Size (Next Tokens) : 27\n"
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "\n",
    "# show top k items\n",
    "top_k = 2\n",
    "\n",
    "# Build Model\n",
    "counts = utils.build_ngram_counts(tokens, order)\n",
    "print(\"Sample counts:\")\n",
    "pprint(list(counts.items())[:top_k])\n",
    "\n",
    "# Print some statistics\n",
    "utils.print_model_statistics(counts, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7666d64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized to proba:\n",
      "[(('a', 'l', 'i'),\n",
      "  {'a': 0.0025, 'c': 0.9851, 's': 0.0025, 't': 0.0025, 'v': 0.0074}),\n",
      " (('l', 'i', 'c'), {'a': 0.0025, 'e': 0.995, 'k': 0.0025})]\n",
      "\n",
      "Most likely token after ('e', 'r', ' ') :\n",
      "   token   proba\n",
      "6      a  0.1408\n",
      "5      t  0.1318\n",
      "1      s  0.1028\n",
      "11     h  0.0950\n",
      "3      w  0.0827\n",
      "0      i  0.0804\n",
      "2      o  0.0503\n",
      "4      f  0.0469\n",
      "12     l  0.0302\n",
      "10     c  0.0279\n",
      "16     e  0.0279\n",
      "8      b  0.0235\n",
      "15     d  0.0201\n",
      "7      m  0.0190\n",
      "14     y  0.0190\n",
      "21     p  0.0156\n",
      "9      g  0.0145\n",
      "18     v  0.0145\n",
      "22     n  0.0145\n",
      "17     q  0.0101\n",
      "20     r  0.0101\n",
      "13     k  0.0089\n",
      "19     u  0.0078\n",
      "23     x  0.0034\n",
      "24     j  0.0022\n"
     ]
    }
   ],
   "source": [
    "# Normalize to Probabilities\n",
    "model = utils.normalize_to_probs(counts)\n",
    "print(\"Normalized to proba:\")\n",
    "pprint(list(model.items())[:top_k])\n",
    "\n",
    "# get context with most diverse next token options\n",
    "context = max(model.keys(), key=lambda ctx: len(model[ctx]))\n",
    "\n",
    "# show sorted next token probabilities for this context\n",
    "df = pd.DataFrame(model[context].items(), columns=[\"token\", \"proba\"]).sort_values(\n",
    "    by=\"proba\", ascending=False\n",
    ")\n",
    "print(f\"\\nMost likely token after {context} :\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "249f43aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Entropy: 1.8276 bits\n"
     ]
    }
   ],
   "source": [
    "# Calculate Entropy (Model based)\n",
    "h_model = utils.calculate_entropy_from_counts(counts)\n",
    "print(f\"Model Entropy: {h_model:.4f} bits\")\n",
    "\n",
    "# def calculate_entropy_from_counts(counts):\n",
    "#     \"\"\"\n",
    "#     H = - Sum_ctx ( P(ctx) * Sum_x ( P(x|ctx) * log2 P(x|ctx) ) )\n",
    "#     \"\"\"\n",
    "#     total_observations = sum(sum(counter.values()) for counter in counts.values())\n",
    "\n",
    "#     entropy = 0.0\n",
    "#     for context, counter in counts.items():\n",
    "#         ctx_count = sum(counter.values())\n",
    "#         p_ctx = ctx_count / total_observations\n",
    "\n",
    "#         h_cond = 0.0\n",
    "#         for token, count in counter.items():\n",
    "#             p_x_given_ctx = count / ctx_count\n",
    "#             h_cond -= p_x_given_ctx * math.log2(p_x_given_ctx)\n",
    "\n",
    "#         entropy += p_ctx * h_cond\n",
    "\n",
    "#     return entropy\n",
    "\n",
    "# The Entropy Rate (or Source Entropy) is a fundamental concept in info theory \n",
    "# that measures the avg amount of uncertainty or information per symbol (token) in a stochastic process (like text).\n",
    "# For a source that generates a sequence of symbols (e.g., characters or words)\n",
    "# the entropy rate, typically denoted $H(\\mathcal{X})$, describes how \"compressible\" the source is.\n",
    "    # Low Entropy Rate: The sequence is highly predictable (redundant). \n",
    "    # Knowing the past tokens tells you a lot about the next one.\n",
    "\n",
    "    # High Entropy Rate: The sequence is less predictable (closer to random). \n",
    "    # Knowing the past doesn't help much in predicting the next token.\n",
    "\n",
    "# -> What it is and what it is not:\n",
    "# ✅ It is H(X_t | X_{t-2},X_{t-1}) for your empirical trigram distribution (with whatever tokenization and handling of boundaries you used).\n",
    "# ❌ It is not the entropy of English in general, and it is not the unconditional entropy of the corpus.\n",
    "# ❌ It is not a guaranteed measure of how well you’ll predict new text. \n",
    "# For held-out data you’d want cross-entropy / perplexity on a test set, because the training-set conditional entropy can be optimistically low \n",
    "# (especially if there are many rare contexts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca59caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Entropy (Sequence based - NLL)\n",
    "# h_seq = utils.calculate_nll(tokens, model, order)\n",
    "# print(f\"Sequence NLL Entropy: {h_seq:.4f} bits\")\n",
    "\n",
    "# When both functions are calculated using the same sequence (i.e., the training sequence is passed as the sequence argument to calculate_nll),\n",
    "# they will indeed give you the same numerical result. \n",
    "# $$H_{\\text{model}} = L_{\\text{train}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "30929993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Text ---\n",
      "boves fining suchnestears faceand to they spers and puttener v adven that evided its and not lar oppeaking oldinah she king in a number alice these against if your sir footmany othis look off were said a\n"
     ]
    }
   ],
   "source": [
    "# Generate Text\n",
    "gen_len = 200\n",
    "generated_tokens = utils.generate_text(model, order, gen_len)\n",
    "\n",
    "if level == \"char\":\n",
    "    gen_text = \"\".join(generated_tokens)\n",
    "else:\n",
    "    gen_text = \" \".join(generated_tokens)\n",
    "\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "605a1ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chapter Analysis...\n",
      "Found 12 chapters.\n",
      "Chapter entropy saved to results/chapter_entropy.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Chapter Analysis (Extension E3) ---\n",
    "print(\"Running Chapter Analysis...\")\n",
    "chapters = utils.split_chapters(raw_text)\n",
    "print(f\"Found {len(chapters)} chapters.\")\n",
    "\n",
    "chapter_results = []\n",
    "# Analyze entropy per chapter for a fixed config (e.g., Char, Punct, k=2)\n",
    "# Ideally we train on the whole text and evaluate NLL on chapters,\n",
    "# OR train on each chapter?\n",
    "# README says \"Evaluate non-stationarity: entropy per chapter.\"\n",
    "# Usually this means measuring the entropy OF that chapter.\n",
    "# We can measure it by:\n",
    "# 1. Training a model on that chapter and finding its entropy.\n",
    "# 2. Or using a global model and finding NLL of that chapter (surprisal).\n",
    "# \"Non-stationarity\" implies the statistics change.\n",
    "# Let's do both? Or just method 1 (simpler interpretation: \"This chapter is more complex\").\n",
    "# Method 2 (Cross-entropy) shows how much the chapter deviates from the global average.\n",
    "# Let's do Method 1 (Self-Entropy) for k=1 (simple)\n",
    "\n",
    "target_config = {\"level\": \"char\", \"keep_punct\": True, \"order\": 1}\n",
    "\n",
    "for i, chapter_text in enumerate(chapters):\n",
    "    tokens = utils.clean_text(\n",
    "        chapter_text,\n",
    "        level=target_config[\"level\"],\n",
    "        keep_punctuation=target_config[\"keep_punct\"],\n",
    "    )\n",
    "    if not tokens:\n",
    "        continue\n",
    "\n",
    "    k = target_config[\"order\"]\n",
    "    counts = utils.build_ngram_counts(tokens, k)\n",
    "    # model = utils.normalize_to_probs(counts) # Not needed for entropy_from_counts\n",
    "\n",
    "    h_chapter = utils.calculate_entropy_from_counts(counts)\n",
    "\n",
    "    chapter_results.append(\n",
    "        {\"chapter\": i + 1, \"entropy\": h_chapter, \"length\": len(tokens)}\n",
    "    )\n",
    "\n",
    "df_chap = pd.DataFrame(chapter_results)\n",
    "df_chap.to_csv(\"results/chapter_entropy.csv\", index=False)\n",
    "print(\"Chapter entropy saved to results/chapter_entropy.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
